{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T19:59:14.488363100Z",
     "start_time": "2025-12-08T19:59:12.980859300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import STOPWORDS\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import sys\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "import networkx as nx\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import base64\n",
    "import os\n",
    "from typing import Dict, Set, Optional\n",
    "import pickle\n",
    "import random\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "BASE = \"https://api.github.com\"\n",
    "TOKEN = os.getenv(\"TOKEN\")\n",
    "HEADERS = {\"Authorization\": f\"token {TOKEN}\"} if TOKEN else {}\n",
    "GRAPH_FILE = \"github_graph.gexf\"\n",
    "SEED_USER = \"gh05tdog\"\n",
    "DEPTH = 2\n",
    "REPO_LIMIT = 40\n",
    "CONTRIBUTOR_LIMIT = 400"
   ],
   "id": "4956125eb2fc52d8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Graph generation\n",
   "id": "7d51167aba8f082c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T19:59:28.199332300Z",
     "start_time": "2025-12-08T19:59:28.167762900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This is for the loggin\n",
    "class GitHubGraphFormatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        # Extract custom attributes if they exist\n",
    "        depth = getattr(record, 'depth', None)\n",
    "        user = getattr(record, 'user', None)\n",
    "        repo = getattr(record, 'repo', None)\n",
    "        time = self.formatTime(record, self.datefmt)\n",
    "\n",
    "        # Create prefix based on available information\n",
    "        prefix = \"\"\n",
    "        if depth is not None:\n",
    "            prefix += f\"[DEPTH:{depth}]\"\n",
    "        if user is not None:\n",
    "            prefix += f\"[USER:{user}]\"\n",
    "        if repo is not None:\n",
    "            prefix += f\"[REPO:{repo}]\"\n",
    "\n",
    "        if prefix:\n",
    "            prefix += \" \"\n",
    "\n",
    "        # Format the message with the prefix\n",
    "        return f\"[{time}] {prefix}{super().format(record)}\"\n",
    "\n",
    "# Configure logger\n",
    "logger = logging.getLogger(\"github_graph\")\n",
    "logger.setLevel(logging.INFO)  # Set to DEBUG to see more detailed logs\n",
    "\n",
    "# Prevent duplicate log messages\n",
    "logger.propagate = False\n",
    "if logger.handlers:\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# Create console handler for INFO and above\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create formatter and add it to the handler\n",
    "formatter = GitHubGraphFormatter('%(levelname)s: %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handler to logger\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Optionally, add a file handler to capture all logs including DEBUG\n",
    "file_handler = logging.FileHandler('github_graph.log', mode='w')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info(\"Logging system initialized\")\n",
    "logger.debug(\"Debug logging enabled - check github_graph.log for detailed logs\")\n",
    "\n",
    "# Function to control log verbosity\n",
    "def set_log_level(level):\n",
    "    if isinstance(level, str):\n",
    "        level = getattr(logging, level.upper())\n",
    "\n",
    "    logger.setLevel(level)\n",
    "    console_handler.setLevel(level)\n",
    "\n",
    "    level_name = logging.getLevelName(level)\n",
    "    logger.info(f\"Console log level set to {level_name}\")\n",
    "\n",
    "\n"
   ],
   "id": "b9aaa5e4db002023",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-08 20:59:28,175] INFO: Logging system initialized\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T19:59:32.594933500Z",
     "start_time": "2025-12-08T19:59:32.558608800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def get_repos(session, user, limit=20):\n",
    "    \"\"\"Fetch repositories owned by a user. And the languages used in each repo.\"\"\"\n",
    "    url = f\"{BASE}/users/{user}/repos\"\n",
    "    logger.debug(f\"Fetching repositories for user '{user}' (limit: {limit})\",\n",
    "                extra={\"user\": user})\n",
    "\n",
    "    async with session.get(url, headers=HEADERS, params={\"per_page\": limit}) as response:\n",
    "        if response.status == 200:\n",
    "            data = await response.json()\n",
    "            repos = [(repo[\"name\"], repo.get(\"language\")) for repo in data]\n",
    "            logger.debug(f\"Successfully fetched {len(repos)} repositories for user '{user}'\",\n",
    "                        extra={\"user\": user})\n",
    "            return repos\n",
    "        elif response.status == 403 and 'X-RateLimit-Remaining' in response.headers and int(response.headers['X-RateLimit-Remaining']) == 0:\n",
    "            logger.error(f\"Rate limit exceeded for user '{user}'\", extra={\"user\": user})\n",
    "            raise RateLimitExceeded(f\"GitHub API rate limit exceeded for user '{user}'\")\n",
    "        else:\n",
    "            logger.warning(f\"Failed to fetch repositories for user '{user}': HTTP {response.status}\",\n",
    "                          extra={\"user\": user})\n",
    "            response.raise_for_status()\n",
    "            return []\n",
    "\n",
    "async def get_contributors(session, user, repo, limit=20):\n",
    "    \"\"\"Fetch contributors for a repository.\"\"\"\n",
    "    url = f\"{BASE}/repos/{user}/{repo}/contributors\"\n",
    "    logger.debug(f\"Fetching contributors for repository '{user}/{repo}' (limit: {limit})\",\n",
    "                extra={\"user\": user, \"repo\": repo})\n",
    "\n",
    "    async with session.get(url, headers=HEADERS, params={\"per_page\": limit}) as response:\n",
    "        if response.status == 200:\n",
    "            data = await response.json()\n",
    "            contributors = [c[\"login\"] for c in data]\n",
    "            logger.debug(f\"Successfully fetched {len(contributors)} contributors for '{user}/{repo}'\",\n",
    "                        extra={\"user\": user, \"repo\": repo})\n",
    "            return contributors\n",
    "        elif response.status == 404:\n",
    "            logger.warning(f\"Repository '{user}/{repo}' not found (HTTP 404)\",\n",
    "                          extra={\"user\": user, \"repo\": repo})\n",
    "            return []\n",
    "        elif response.status == 403 and 'X-RateLimit-Remaining' in response.headers and int(response.headers['X-RateLimit-Remaining']) == 0:\n",
    "            logger.error(f\"Rate limit exceeded for repository '{user}/{repo}'\", extra={\"user\": user, \"repo\": repo})\n",
    "            raise RateLimitExceeded(f\"GitHub API rate limit exceeded for repository '{user}/{repo}'\")\n",
    "        else:\n",
    "            logger.warning(f\"Failed to fetch contributors for '{user}/{repo}': HTTP {response.status}\",\n",
    "                          extra={\"user\": user, \"repo\": repo})\n",
    "            response.raise_for_status()\n",
    "            return []\n",
    "\n",
    "# Custom exception for rate limit\n",
    "class RateLimitExceeded(Exception):\n",
    "    \"\"\"Exception raised when GitHub API rate limit is exceeded.\"\"\"\n",
    "    pass\n"
   ],
   "id": "853c6646b87c87a4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T19:59:34.077091Z",
     "start_time": "2025-12-08T19:59:34.050475800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_graph(G, graph_file):\n",
    "    \"\"\"Save the graph to a file, converting set attributes to strings for GEXF compatibility.\"\"\"\n",
    "    if not graph_file:\n",
    "        graph_file = GRAPH_FILE\n",
    "\n",
    "    logger.info(f\"Saving current progress to {graph_file}\")\n",
    "    G_save = G.copy()\n",
    "\n",
    "    for node, data in G_save.nodes(data=True):\n",
    "        for key, val in list(data.items()):\n",
    "            if isinstance(val, set):\n",
    "                data[key] = \", \".join(sorted(val))\n",
    "\n",
    "    for node, data in G_save.nodes(data=True):\n",
    "        G_save.nodes[node][\"label\"] = node\n",
    "        if isinstance(data.get(\"repos\"), set):\n",
    "            G_save.nodes[node][\"repos\"] = \", \".join(sorted(data[\"repos\"]))\n",
    "        elif \"repos\" not in data:\n",
    "            G_save.nodes[node][\"repos\"] = \"\"\n",
    "\n",
    "    nx.write_gexf(G_save, graph_file)\n",
    "    logger.info(f\"Saved graph with {len(G_save.nodes())} nodes and {len(G_save.edges())} edges to {graph_file}\")\n",
    "    return G_save\n"
   ],
   "id": "944a8fb46e10b4f4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T19:59:35.189145500Z",
     "start_time": "2025-12-08T19:59:35.160633900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_language_map(user_languages, filename=\"user_languages.json\"):\n",
    "    \"\"\"Save the user->languages mapping to a JSON file.\"\"\"\n",
    "\n",
    "    # Convert sets to lists for JSON serialization\n",
    "    language_dict = {user: list(langs) for user, langs in user_languages.items()}\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(language_dict, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Saved language mapping for {len(language_dict)} users to {filename}\")"
   ],
   "id": "9f47c37d0948a453",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T19:59:37.882501600Z",
     "start_time": "2025-12-08T19:59:37.855812800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def build_graph(seed_user, depth=2, repo_limit=10, contrib_limit=40, graph_file=None):\n",
    "    # Start fresh\n",
    "    G = nx.Graph()\n",
    "    visited = set()\n",
    "    exploration_queue = [(seed_user, 0)]  # (user, depth)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Starting graph building with seed user: {seed_user}\",\n",
    "        extra={\"user\": seed_user}\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        f\"Parameters - Depth: {depth}, Repo limit: {repo_limit}, Contributor limit: {contrib_limit}\"\n",
    "    )\n",
    "\n",
    "    # Store user languages\n",
    "    user_languages = {}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Process the exploration queue\n",
    "        while exploration_queue:\n",
    "            user, d = exploration_queue.pop(0)\n",
    "\n",
    "            # Skip if already visited or beyond depth limit\n",
    "            if user in visited or d > depth:\n",
    "                continue\n",
    "\n",
    "            # Skip bots immediately\n",
    "            if \"[bot]\" in user.lower():\n",
    "                logger.debug(f\"Skipping bot user '{user}' entirely\")\n",
    "                continue\n",
    "\n",
    "            visited.add(user)\n",
    "            indent = \"  \" * d\n",
    "            logger.info(f\"{indent}Exploring user '{user}' at depth {d}\",\n",
    "                        extra={\"depth\": d, \"user\": user})\n",
    "\n",
    "            # Fetch repositories for this user\n",
    "            try:\n",
    "                repos = await get_repos(session, user, repo_limit)\n",
    "                logger.info(f\"{indent}Found {len(repos)} repositories for user '{user}'\",\n",
    "                extra={\"depth\": d, \"user\": user})\n",
    "            except RateLimitExceeded as e:\n",
    "                logger.error(f\"{indent}Rate limit exceeded for user '{user}': {str(e)}\")\n",
    "                # Save state for resuming\n",
    "\n",
    "                save_graph(G, graph_file)\n",
    "                return G\n",
    "            except aiohttp.ClientError as e:\n",
    "                logger.error(f\"{indent}Failed to fetch repositories for user '{user}': {str(e)}\")\n",
    "                continue\n",
    "\n",
    "            if user not in user_languages:\n",
    "                user_languages[user] = set()\n",
    "\n",
    "            # Process each repository\n",
    "            for repo_name, language in repos:\n",
    "                full_repo = f\"{user}/{repo_name}\"\n",
    "\n",
    "                # Track language for the repo owner\n",
    "                if language:\n",
    "                    user_languages[user].add(language)\n",
    "\n",
    "                logger.info(f\"{indent}Processing repository: {full_repo} (Language: {language})\",\n",
    "                            extra={\"depth\": d, \"user\": user, \"repo\": full_repo})\n",
    "\n",
    "                try:\n",
    "                    contributors = await get_contributors(session, user, repo_name, contrib_limit)\n",
    "                    logger.info(f\"{indent}Found {len(contributors)} contributors for {full_repo}\",\n",
    "                                extra={\"depth\": d, \"user\": user, \"repo\": full_repo})\n",
    "                except RateLimitExceeded as e:\n",
    "                    logger.error(f\"{indent}Rate limit exceeded for repository '{full_repo}': {str(e)}\")\n",
    "                    # Save state for resuming\n",
    "                    save_graph(G, graph_file)\n",
    "                    return G\n",
    "                except aiohttp.ClientError as e:\n",
    "                    logger.warning(f\"{indent}Failed to fetch contributors for {full_repo}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "                # Filter out bots right away\n",
    "                human_contributors = [c for c in contributors if \"[bot]\" not in c.lower()]\n",
    "                if len(human_contributors) < len(contributors):\n",
    "                    logger.debug(f\"{indent}Skipped {len(contributors) - len(human_contributors)} bots in {full_repo}\")\n",
    "\n",
    "                # Track languages for contributors\n",
    "                for c in human_contributors:\n",
    "                    if c not in user_languages:\n",
    "                        user_languages[c] = set()\n",
    "                    if language:\n",
    "                        user_languages[c].add(language)\n",
    "\n",
    "                # Add contributors as nodes\n",
    "                nodes_added = 0\n",
    "                for c in human_contributors:\n",
    "                    if c not in G:\n",
    "                        G.add_node(c, repos=set())\n",
    "                        nodes_added += 1\n",
    "                    G.nodes[c][\"repos\"].add(full_repo)\n",
    "\n",
    "                if nodes_added > 0:\n",
    "                    logger.info(f\"{indent}Added {nodes_added} new nodes for repository {full_repo}\")\n",
    "\n",
    "                # Connect collaborators (undirected)\n",
    "                edges_added = 0\n",
    "                for i, c1 in enumerate(human_contributors):\n",
    "                    for c2 in human_contributors[i + 1:]:\n",
    "                        G.add_edge(c1, c2, repo=full_repo)\n",
    "                        edges_added += 1\n",
    "\n",
    "                if edges_added > 0:\n",
    "                    logger.info(f\"{indent}Added {edges_added} edges for repository {full_repo}\")\n",
    "\n",
    "                # Add contributors to the exploration queue for the next depth level\n",
    "                for contributor in human_contributors:\n",
    "                    if contributor not in visited:\n",
    "                        exploration_queue.append((contributor, d + 1))\n",
    "\n",
    "            # Periodically save progress\n",
    "            if len(visited) % 10 == 0:\n",
    "                save_graph(G, graph_file)\n",
    "\n",
    "    logger.info(f\"Graph building completed. Final graph has {len(G.nodes())} nodes and {len(G.edges())} edges.\")\n",
    "\n",
    "    # Save final state\n",
    "    save_graph(G, graph_file)\n",
    "\n",
    "    return G, user_languages\n"
   ],
   "id": "c5c4ce104f2ccf4e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T19:59:42.491179200Z",
     "start_time": "2025-12-08T19:59:42.456241900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define an async function to run the graph building process\n",
    "async def run_graph_building():\n",
    "    if os.path.exists(GRAPH_FILE):\n",
    "        logger.info(f\"Loading existing graph from {GRAPH_FILE}\")\n",
    "        G = nx.read_gexf(GRAPH_FILE)\n",
    "        user_languages = {}  # Empty dict if loading existing graph\n",
    "        logger.warning(\"Loaded existing graph without language data. Run a fresh build to collect language information.\")\n",
    "    # Start fresh\n",
    "    else:\n",
    "        logger.info(\"Building new graph...\")\n",
    "        G, user_languages = await build_graph(SEED_USER, depth=DEPTH, contrib_limit=CONTRIBUTOR_LIMIT,\n",
    "                             repo_limit=REPO_LIMIT, graph_file=GRAPH_FILE)\n",
    "\n",
    "        # If the graph was saved during building due to rate limit, it's already in the right format\n",
    "        # Otherwise, save it now\n",
    "        if not os.path.exists(GRAPH_FILE):\n",
    "            G_save = save_graph(G, GRAPH_FILE)\n",
    "            G = G_save\n",
    "\n",
    "    return G, user_languages\n"
   ],
   "id": "746d8f93da337981",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T20:00:01.085108400Z",
     "start_time": "2025-12-08T19:59:46.141102900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the async function using asyncio\n",
    "try:\n",
    "    G, user_languages = asyncio.run(run_graph_building())\n",
    "    save_language_map(user_languages)\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"Process interrupted by user. Progress has been saved and can be resumed.\")\n",
    "    if os.path.exists(GRAPH_FILE):\n",
    "        logger.info(f\"Loading saved graph from {GRAPH_FILE}\")\n",
    "        G = nx.read_gexf(GRAPH_FILE)\n",
    "    else:\n",
    "        logger.error(\"No saved graph. Exiting.\")\n",
    "        raise\n",
    "\n",
    "logger.info(f\"Graph statistics: {len(G.nodes())} nodes, {len(G.edges())} edges\")\n"
   ],
   "id": "f283c5bf37a4e4d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-08 20:59:46,145] INFO: Building new graph...\n",
      "[2025-12-08 20:59:46,145] [USER:gh05tdog] INFO: Starting graph building with seed user: gh05tdog\n",
      "[2025-12-08 20:59:46,146] INFO: Parameters - Depth: 2, Repo limit: 40, Contributor limit: 400\n",
      "[2025-12-08 20:59:46,146] [DEPTH:0][USER:gh05tdog] INFO: Exploring user 'gh05tdog' at depth 0\n",
      "[2025-12-08 20:59:46,638] [DEPTH:0][USER:gh05tdog] INFO: Found 13 repositories for user 'gh05tdog'\n",
      "[2025-12-08 20:59:46,639] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/DAPM_Master_Thesis_Group_D] INFO: Processing repository: gh05tdog/DAPM_Master_Thesis_Group_D (Language: C#)\n",
      "[2025-12-08 20:59:46,909] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/DAPM_Master_Thesis_Group_D] INFO: Found 11 contributors for gh05tdog/DAPM_Master_Thesis_Group_D\n",
      "[2025-12-08 20:59:46,910] INFO: Added 11 new nodes for repository gh05tdog/DAPM_Master_Thesis_Group_D\n",
      "[2025-12-08 20:59:46,910] INFO: Added 55 edges for repository gh05tdog/DAPM_Master_Thesis_Group_D\n",
      "[2025-12-08 20:59:46,911] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/gh05tdog] INFO: Processing repository: gh05tdog/gh05tdog (Language: None)\n",
      "[2025-12-08 20:59:47,133] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/gh05tdog] INFO: Found 1 contributors for gh05tdog/gh05tdog\n",
      "[2025-12-08 20:59:47,134] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/Group10-02148] INFO: Processing repository: gh05tdog/Group10-02148 (Language: Java)\n",
      "[2025-12-08 20:59:47,403] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/Group10-02148] INFO: Found 4 contributors for gh05tdog/Group10-02148\n",
      "[2025-12-08 20:59:47,404] INFO: Added 1 new nodes for repository gh05tdog/Group10-02148\n",
      "[2025-12-08 20:59:47,404] INFO: Added 6 edges for repository gh05tdog/Group10-02148\n",
      "[2025-12-08 20:59:47,405] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/group_19_cell_discovery] INFO: Processing repository: gh05tdog/group_19_cell_discovery (Language: C)\n",
      "[2025-12-08 20:59:47,603] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/group_19_cell_discovery] INFO: Found 3 contributors for gh05tdog/group_19_cell_discovery\n",
      "[2025-12-08 20:59:47,604] INFO: Added 3 edges for repository gh05tdog/group_19_cell_discovery\n",
      "[2025-12-08 20:59:47,604] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/HorseStore] INFO: Processing repository: gh05tdog/HorseStore (Language: Java)\n",
      "[2025-12-08 20:59:47,801] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/HorseStore] INFO: Found 1 contributors for gh05tdog/HorseStore\n",
      "[2025-12-08 20:59:47,802] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/iceleaderboard] INFO: Processing repository: gh05tdog/iceleaderboard (Language: HTML)\n",
      "[2025-12-08 20:59:48,004] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/iceleaderboard] INFO: Found 1 contributors for gh05tdog/iceleaderboard\n",
      "[2025-12-08 20:59:48,004] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/iceTilbud] INFO: Processing repository: gh05tdog/iceTilbud (Language: JavaScript)\n",
      "[2025-12-08 20:59:48,221] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/iceTilbud] INFO: Found 1 contributors for gh05tdog/iceTilbud\n",
      "[2025-12-08 20:59:48,221] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/jpamb] INFO: Processing repository: gh05tdog/jpamb (Language: Python)\n",
      "[2025-12-08 20:59:48,437] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/jpamb] INFO: Found 9 contributors for gh05tdog/jpamb\n",
      "[2025-12-08 20:59:48,438] INFO: Added 8 new nodes for repository gh05tdog/jpamb\n",
      "[2025-12-08 20:59:48,438] INFO: Added 36 edges for repository gh05tdog/jpamb\n",
      "[2025-12-08 20:59:48,439] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/Login] INFO: Processing repository: gh05tdog/Login (Language: Java)\n",
      "[2025-12-08 20:59:48,657] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/Login] INFO: Found 1 contributors for gh05tdog/Login\n",
      "[2025-12-08 20:59:48,657] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/projekt_19] INFO: Processing repository: gh05tdog/projekt_19 (Language: Java)\n",
      "[2025-12-08 20:59:48,854] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/projekt_19] INFO: Found 4 contributors for gh05tdog/projekt_19\n",
      "[2025-12-08 20:59:48,855] INFO: Added 1 new nodes for repository gh05tdog/projekt_19\n",
      "[2025-12-08 20:59:48,855] INFO: Added 6 edges for repository gh05tdog/projekt_19\n",
      "[2025-12-08 20:59:48,856] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/rally] INFO: Processing repository: gh05tdog/rally (Language: None)\n",
      "[2025-12-08 20:59:49,133] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/rally] INFO: Found 81 contributors for gh05tdog/rally\n",
      "[2025-12-08 20:59:49,133] INFO: Added 80 new nodes for repository gh05tdog/rally\n",
      "[2025-12-08 20:59:49,138] INFO: Added 3160 edges for repository gh05tdog/rally\n",
      "[2025-12-08 20:59:49,138] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/Sudoku_Solver] INFO: Processing repository: gh05tdog/Sudoku_Solver (Language: Java)\n",
      "[2025-12-08 20:59:49,365] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/Sudoku_Solver] INFO: Found 3 contributors for gh05tdog/Sudoku_Solver\n",
      "[2025-12-08 20:59:49,366] INFO: Added 3 edges for repository gh05tdog/Sudoku_Solver\n",
      "[2025-12-08 20:59:49,366] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/terningedrink] INFO: Processing repository: gh05tdog/terningedrink (Language: JavaScript)\n",
      "[2025-12-08 20:59:49,567] [DEPTH:0][USER:gh05tdog][REPO:gh05tdog/terningedrink] INFO: Found 1 contributors for gh05tdog/terningedrink\n",
      "[2025-12-08 20:59:49,567] [DEPTH:1][USER:RasmusUK] INFO:   Exploring user 'RasmusUK' at depth 1\n",
      "[2025-12-08 20:59:50,000] [DEPTH:1][USER:RasmusUK] INFO:   Found 21 repositories for user 'RasmusUK'\n",
      "[2025-12-08 20:59:50,001] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/02805-social-graphs-and-interactions-project] INFO:   Processing repository: RasmusUK/02805-social-graphs-and-interactions-project (Language: Jupyter Notebook)\n",
      "[2025-12-08 20:59:50,234] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/02805-social-graphs-and-interactions-project] INFO:   Found 2 contributors for RasmusUK/02805-social-graphs-and-interactions-project\n",
      "[2025-12-08 20:59:50,234] INFO:   Added 1 edges for repository RasmusUK/02805-social-graphs-and-interactions-project\n",
      "[2025-12-08 20:59:50,234] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/Advent-of-Code-2023] INFO:   Processing repository: RasmusUK/Advent-of-Code-2023 (Language: C#)\n",
      "[2025-12-08 20:59:50,455] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/Advent-of-Code-2023] INFO:   Found 1 contributors for RasmusUK/Advent-of-Code-2023\n",
      "[2025-12-08 20:59:50,456] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/Advent-of-Code-2024] INFO:   Processing repository: RasmusUK/Advent-of-Code-2024 (Language: C#)\n",
      "[2025-12-08 20:59:50,725] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/Advent-of-Code-2024] INFO:   Found 1 contributors for RasmusUK/Advent-of-Code-2024\n",
      "[2025-12-08 20:59:50,726] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/AdventOfCode2022] INFO:   Processing repository: RasmusUK/AdventOfCode2022 (Language: C#)\n",
      "[2025-12-08 20:59:50,960] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/AdventOfCode2022] INFO:   Found 1 contributors for RasmusUK/AdventOfCode2022\n",
      "[2025-12-08 20:59:50,960] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/Bash] INFO:   Processing repository: RasmusUK/Bash (Language: Shell)\n",
      "[2025-12-08 20:59:51,168] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/Bash] INFO:   Found 1 contributors for RasmusUK/Bash\n",
      "[2025-12-08 20:59:51,168] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/BDSA_Assignment_0] INFO:   Processing repository: RasmusUK/BDSA_Assignment_0 (Language: TeX)\n",
      "[2025-12-08 20:59:51,370] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/BDSA_Assignment_0] INFO:   Found 1 contributors for RasmusUK/BDSA_Assignment_0\n",
      "[2025-12-08 20:59:51,370] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/BDSA_Assignment_0_Again] INFO:   Processing repository: RasmusUK/BDSA_Assignment_0_Again (Language: None)\n",
      "[2025-12-08 20:59:51,572] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/BDSA_Assignment_0_Again] INFO:   Found 1 contributors for RasmusUK/BDSA_Assignment_0_Again\n",
      "[2025-12-08 20:59:51,573] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/BDSA_Assignment_5] INFO:   Processing repository: RasmusUK/BDSA_Assignment_5 (Language: C#)\n",
      "[2025-12-08 20:59:51,767] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/BDSA_Assignment_5] INFO:   Found 2 contributors for RasmusUK/BDSA_Assignment_5\n",
      "[2025-12-08 20:59:51,768] INFO:   Added 1 new nodes for repository RasmusUK/BDSA_Assignment_5\n",
      "[2025-12-08 20:59:51,768] INFO:   Added 1 edges for repository RasmusUK/BDSA_Assignment_5\n",
      "[2025-12-08 20:59:51,769] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/BisserupWASM] INFO:   Processing repository: RasmusUK/BisserupWASM (Language: C#)\n",
      "[2025-12-08 20:59:51,974] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/BisserupWASM] INFO:   Found 1 contributors for RasmusUK/BisserupWASM\n",
      "[2025-12-08 20:59:51,974] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/BlazorScheduler] INFO:   Processing repository: RasmusUK/BlazorScheduler (Language: None)\n",
      "[2025-12-08 20:59:52,198] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/BlazorScheduler] INFO:   Found 4 contributors for RasmusUK/BlazorScheduler\n",
      "[2025-12-08 20:59:52,199] INFO:   Added 3 new nodes for repository RasmusUK/BlazorScheduler\n",
      "[2025-12-08 20:59:52,199] INFO:   Added 3 edges for repository RasmusUK/BlazorScheduler\n",
      "[2025-12-08 20:59:52,200] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/DISYS_E02] INFO:   Processing repository: RasmusUK/DISYS_E02 (Language: Go)\n",
      "[2025-12-08 20:59:52,376] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/DISYS_E02] INFO:   Found 0 contributors for RasmusUK/DISYS_E02\n",
      "[2025-12-08 20:59:52,377] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/DISYS_Mini_Project_1] INFO:   Processing repository: RasmusUK/DISYS_Mini_Project_1 (Language: Go)\n",
      "[2025-12-08 20:59:52,575] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/DISYS_Mini_Project_1] INFO:   Found 2 contributors for RasmusUK/DISYS_Mini_Project_1\n",
      "[2025-12-08 20:59:52,576] INFO:   Added 1 new nodes for repository RasmusUK/DISYS_Mini_Project_1\n",
      "[2025-12-08 20:59:52,576] INFO:   Added 1 edges for repository RasmusUK/DISYS_Mini_Project_1\n",
      "[2025-12-08 20:59:52,577] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/DISYS_Mini_Project_3] INFO:   Processing repository: RasmusUK/DISYS_Mini_Project_3 (Language: Go)\n",
      "[2025-12-08 20:59:52,774] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/DISYS_Mini_Project_3] INFO:   Found 2 contributors for RasmusUK/DISYS_Mini_Project_3\n",
      "[2025-12-08 20:59:52,775] INFO:   Added 1 edges for repository RasmusUK/DISYS_Mini_Project_3\n",
      "[2025-12-08 20:59:52,775] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/DISYS_Mock_Exam] INFO:   Processing repository: RasmusUK/DISYS_Mock_Exam (Language: Go)\n",
      "[2025-12-08 20:59:52,973] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/DISYS_Mock_Exam] INFO:   Found 1 contributors for RasmusUK/DISYS_Mock_Exam\n",
      "[2025-12-08 20:59:52,974] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/IAI_Configuration_Project] INFO:   Processing repository: RasmusUK/IAI_Configuration_Project (Language: HTML)\n",
      "[2025-12-08 20:59:53,167] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/IAI_Configuration_Project] INFO:   Found 2 contributors for RasmusUK/IAI_Configuration_Project\n",
      "[2025-12-08 20:59:53,168] INFO:   Added 1 edges for repository RasmusUK/IAI_Configuration_Project\n",
      "[2025-12-08 20:59:53,168] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/java-web-tasks] INFO:   Processing repository: RasmusUK/java-web-tasks (Language: None)\n",
      "[2025-12-08 20:59:53,355] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/java-web-tasks] INFO:   Found 1 contributors for RasmusUK/java-web-tasks\n",
      "[2025-12-08 20:59:53,355] INFO:   Added 1 new nodes for repository RasmusUK/java-web-tasks\n",
      "[2025-12-08 20:59:53,356] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/learn-c-the-hard-way-lectures] INFO:   Processing repository: RasmusUK/learn-c-the-hard-way-lectures (Language: None)\n",
      "[2025-12-08 20:59:53,558] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/learn-c-the-hard-way-lectures] INFO:   Found 2 contributors for RasmusUK/learn-c-the-hard-way-lectures\n",
      "[2025-12-08 20:59:53,559] INFO:   Added 2 new nodes for repository RasmusUK/learn-c-the-hard-way-lectures\n",
      "[2025-12-08 20:59:53,559] INFO:   Added 1 edges for repository RasmusUK/learn-c-the-hard-way-lectures\n",
      "[2025-12-08 20:59:53,560] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/Master-Thesis] INFO:   Processing repository: RasmusUK/Master-Thesis (Language: HTML)\n",
      "[2025-12-08 20:59:53,770] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/Master-Thesis] INFO:   Found 1 contributors for RasmusUK/Master-Thesis\n",
      "[2025-12-08 20:59:53,771] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/ScoreCard] INFO:   Processing repository: RasmusUK/ScoreCard (Language: TypeScript)\n",
      "[2025-12-08 20:59:53,978] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/ScoreCard] INFO:   Found 1 contributors for RasmusUK/ScoreCard\n",
      "[2025-12-08 20:59:53,979] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/Scrabble] INFO:   Processing repository: RasmusUK/Scrabble (Language: F#)\n",
      "[2025-12-08 20:59:54,289] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/Scrabble] INFO:   Found 0 contributors for RasmusUK/Scrabble\n",
      "[2025-12-08 20:59:54,290] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/workout-tracking-app] INFO:   Processing repository: RasmusUK/workout-tracking-app (Language: Kotlin)\n",
      "[2025-12-08 20:59:54,490] [DEPTH:1][USER:RasmusUK][REPO:RasmusUK/workout-tracking-app] INFO:   Found 1 contributors for RasmusUK/workout-tracking-app\n",
      "[2025-12-08 20:59:54,491] [DEPTH:1][USER:sergidoce] INFO:   Exploring user 'sergidoce' at depth 1\n",
      "[2025-12-08 20:59:54,809] [DEPTH:1][USER:sergidoce] INFO:   Found 8 repositories for user 'sergidoce'\n",
      "[2025-12-08 20:59:54,810] [DEPTH:1][USER:sergidoce][REPO:sergidoce/DAPM_Master_Thesis] INFO:   Processing repository: sergidoce/DAPM_Master_Thesis (Language: C#)\n",
      "[2025-12-08 20:59:55,016] [DEPTH:1][USER:sergidoce][REPO:sergidoce/DAPM_Master_Thesis] INFO:   Found 2 contributors for sergidoce/DAPM_Master_Thesis\n",
      "[2025-12-08 20:59:55,017] INFO:   Added 1 edges for repository sergidoce/DAPM_Master_Thesis\n",
      "[2025-12-08 20:59:55,017] [DEPTH:1][USER:sergidoce][REPO:sergidoce/generador] INFO:   Processing repository: sergidoce/generador (Language: Python)\n",
      "[2025-12-08 20:59:55,208] [DEPTH:1][USER:sergidoce][REPO:sergidoce/generador] INFO:   Found 1 contributors for sergidoce/generador\n",
      "[2025-12-08 20:59:55,209] [DEPTH:1][USER:sergidoce][REPO:sergidoce/lacapsadetrons] INFO:   Processing repository: sergidoce/lacapsadetrons (Language: CSS)\n",
      "[2025-12-08 20:59:55,480] [DEPTH:1][USER:sergidoce][REPO:sergidoce/lacapsadetrons] INFO:   Found 1 contributors for sergidoce/lacapsadetrons\n",
      "[2025-12-08 20:59:55,481] [DEPTH:1][USER:sergidoce][REPO:sergidoce/LinkfireTask] INFO:   Processing repository: sergidoce/LinkfireTask (Language: C#)\n",
      "[2025-12-08 20:59:55,666] [DEPTH:1][USER:sergidoce][REPO:sergidoce/LinkfireTask] INFO:   Found 1 contributors for sergidoce/LinkfireTask\n",
      "[2025-12-08 20:59:55,667] [DEPTH:1][USER:sergidoce][REPO:sergidoce/LiRA-Visualization-Framework--Processing-Pipeline] INFO:   Processing repository: sergidoce/LiRA-Visualization-Framework--Processing-Pipeline (Language: Python)\n",
      "[2025-12-08 20:59:55,877] [DEPTH:1][USER:sergidoce][REPO:sergidoce/LiRA-Visualization-Framework--Processing-Pipeline] INFO:   Found 1 contributors for sergidoce/LiRA-Visualization-Framework--Processing-Pipeline\n",
      "[2025-12-08 20:59:55,878] [DEPTH:1][USER:sergidoce][REPO:sergidoce/portfolio] INFO:   Processing repository: sergidoce/portfolio (Language: CSS)\n",
      "[2025-12-08 20:59:56,105] [DEPTH:1][USER:sergidoce][REPO:sergidoce/portfolio] INFO:   Found 1 contributors for sergidoce/portfolio\n",
      "[2025-12-08 20:59:56,105] [DEPTH:1][USER:sergidoce][REPO:sergidoce/pyTelegramBotAPI] INFO:   Processing repository: sergidoce/pyTelegramBotAPI (Language: None)\n",
      "[2025-12-08 20:59:56,436] [DEPTH:1][USER:sergidoce][REPO:sergidoce/pyTelegramBotAPI] INFO:   Found 88 contributors for sergidoce/pyTelegramBotAPI\n",
      "[2025-12-08 20:59:56,437] INFO:   Added 87 new nodes for repository sergidoce/pyTelegramBotAPI\n",
      "[2025-12-08 20:59:56,440] INFO:   Added 3741 edges for repository sergidoce/pyTelegramBotAPI\n",
      "[2025-12-08 20:59:56,441] [DEPTH:1][USER:sergidoce][REPO:sergidoce/sergidoce] INFO:   Processing repository: sergidoce/sergidoce (Language: None)\n",
      "[2025-12-08 20:59:56,609] [USER:sergidoce][REPO:sergidoce] WARNING: Failed to fetch contributors for 'sergidoce/sergidoce': HTTP 204\n",
      "[2025-12-08 20:59:56,609] [DEPTH:1][USER:sergidoce][REPO:sergidoce/sergidoce] INFO:   Found 0 contributors for sergidoce/sergidoce\n",
      "[2025-12-08 20:59:56,610] [DEPTH:1][USER:HLLissau] INFO:   Exploring user 'HLLissau' at depth 1\n",
      "[2025-12-08 20:59:56,998] [DEPTH:1][USER:HLLissau] INFO:   Found 10 repositories for user 'HLLissau'\n",
      "[2025-12-08 20:59:56,998] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Autonomous_robots] INFO:   Processing repository: HLLissau/Autonomous_robots (Language: C)\n",
      "[2025-12-08 20:59:57,235] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Autonomous_robots] INFO:   Found 4 contributors for HLLissau/Autonomous_robots\n",
      "[2025-12-08 20:59:57,236] INFO:   Added 3 new nodes for repository HLLissau/Autonomous_robots\n",
      "[2025-12-08 20:59:57,236] INFO:   Added 6 edges for repository HLLissau/Autonomous_robots\n",
      "[2025-12-08 20:59:57,237] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Bachelor-tsch] INFO:   Processing repository: HLLissau/Bachelor-tsch (Language: C)\n",
      "[2025-12-08 20:59:57,678] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Bachelor-tsch] INFO:   Found 100 contributors for HLLissau/Bachelor-tsch\n",
      "[2025-12-08 20:59:57,679] INFO:   Added 98 new nodes for repository HLLissau/Bachelor-tsch\n",
      "[2025-12-08 20:59:57,683] INFO:   Added 4851 edges for repository HLLissau/Bachelor-tsch\n",
      "[2025-12-08 20:59:57,684] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Diabetes_Group13] INFO:   Processing repository: HLLissau/Diabetes_Group13 (Language: Java)\n",
      "[2025-12-08 20:59:57,932] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Diabetes_Group13] INFO:   Found 4 contributors for HLLissau/Diabetes_Group13\n",
      "[2025-12-08 20:59:57,933] INFO:   Added 3 new nodes for repository HLLissau/Diabetes_Group13\n",
      "[2025-12-08 20:59:57,933] INFO:   Added 6 edges for repository HLLissau/Diabetes_Group13\n",
      "[2025-12-08 20:59:57,933] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Final_assignment] INFO:   Processing repository: HLLissau/Final_assignment (Language: C++)\n",
      "[2025-12-08 20:59:58,133] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Final_assignment] INFO:   Found 0 contributors for HLLissau/Final_assignment\n",
      "[2025-12-08 20:59:58,134] [DEPTH:1][USER:HLLissau][REPO:HLLissau/hyggec] INFO:   Processing repository: HLLissau/hyggec (Language: F#)\n",
      "[2025-12-08 20:59:58,325] [DEPTH:1][USER:HLLissau][REPO:HLLissau/hyggec] INFO:   Found 1 contributors for HLLissau/hyggec\n",
      "[2025-12-08 20:59:58,326] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Mandelbroth] INFO:   Processing repository: HLLissau/Mandelbroth (Language: Java)\n",
      "[2025-12-08 20:59:58,528] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Mandelbroth] INFO:   Found 1 contributors for HLLissau/Mandelbroth\n",
      "[2025-12-08 20:59:58,529] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Minesweeper] INFO:   Processing repository: HLLissau/Minesweeper (Language: Java)\n",
      "[2025-12-08 20:59:58,757] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Minesweeper] INFO:   Found 3 contributors for HLLissau/Minesweeper\n",
      "[2025-12-08 20:59:58,758] INFO:   Added 1 new nodes for repository HLLissau/Minesweeper\n",
      "[2025-12-08 20:59:58,758] INFO:   Added 3 edges for repository HLLissau/Minesweeper\n",
      "[2025-12-08 20:59:58,758] [DEPTH:1][USER:HLLissau][REPO:HLLissau/MinesweeperAdvanced] INFO:   Processing repository: HLLissau/MinesweeperAdvanced (Language: Java)\n",
      "[2025-12-08 20:59:58,964] [DEPTH:1][USER:HLLissau][REPO:HLLissau/MinesweeperAdvanced] INFO:   Found 3 contributors for HLLissau/MinesweeperAdvanced\n",
      "[2025-12-08 20:59:58,964] INFO:   Added 3 edges for repository HLLissau/MinesweeperAdvanced\n",
      "[2025-12-08 20:59:58,965] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Project] INFO:   Processing repository: HLLissau/Project (Language: C++)\n",
      "[2025-12-08 20:59:59,187] [DEPTH:1][USER:HLLissau][REPO:HLLissau/Project] INFO:   Found 2 contributors for HLLissau/Project\n",
      "[2025-12-08 20:59:59,188] INFO:   Added 1 new nodes for repository HLLissau/Project\n",
      "[2025-12-08 20:59:59,188] INFO:   Added 1 edges for repository HLLissau/Project\n",
      "[2025-12-08 20:59:59,189] [DEPTH:1][USER:HLLissau][REPO:HLLissau/SoftwareProjekt] INFO:   Processing repository: HLLissau/SoftwareProjekt (Language: Java)\n",
      "[2025-12-08 20:59:59,398] [DEPTH:1][USER:HLLissau][REPO:HLLissau/SoftwareProjekt] INFO:   Found 3 contributors for HLLissau/SoftwareProjekt\n",
      "[2025-12-08 20:59:59,399] INFO:   Added 3 edges for repository HLLissau/SoftwareProjekt\n",
      "[2025-12-08 20:59:59,399] [DEPTH:1][USER:Martin-Surlykke] INFO:   Exploring user 'Martin-Surlykke' at depth 1\n",
      "[2025-12-08 20:59:59,604] [DEPTH:1][USER:Martin-Surlykke] INFO:   Found 3 repositories for user 'Martin-Surlykke'\n",
      "[2025-12-08 20:59:59,605] [DEPTH:1][USER:Martin-Surlykke][REPO:Martin-Surlykke/Analyzer] INFO:   Processing repository: Martin-Surlykke/Analyzer (Language: None)\n",
      "[2025-12-08 20:59:59,790] [USER:Martin-Surlykke][REPO:Analyzer] WARNING: Failed to fetch contributors for 'Martin-Surlykke/Analyzer': HTTP 204\n",
      "[2025-12-08 20:59:59,791] [DEPTH:1][USER:Martin-Surlykke][REPO:Martin-Surlykke/Analyzer] INFO:   Found 0 contributors for Martin-Surlykke/Analyzer\n",
      "[2025-12-08 20:59:59,792] [DEPTH:1][USER:Martin-Surlykke][REPO:Martin-Surlykke/EA_framework] INFO:   Processing repository: Martin-Surlykke/EA_framework (Language: Java)\n",
      "[2025-12-08 20:59:59,993] [DEPTH:1][USER:Martin-Surlykke][REPO:Martin-Surlykke/EA_framework] INFO:   Found 1 contributors for Martin-Surlykke/EA_framework\n",
      "[2025-12-08 20:59:59,993] [DEPTH:1][USER:Martin-Surlykke][REPO:Martin-Surlykke/ML_assignment_1] INFO:   Processing repository: Martin-Surlykke/ML_assignment_1 (Language: Python)\n",
      "[2025-12-08 21:00:00,020] [USER:Martin-Surlykke][REPO:ML_assignment_1] ERROR: Rate limit exceeded for repository 'Martin-Surlykke/ML_assignment_1'\n",
      "[2025-12-08 21:00:00,021] ERROR:   Rate limit exceeded for repository 'Martin-Surlykke/ML_assignment_1': GitHub API rate limit exceeded for repository 'Martin-Surlykke/ML_assignment_1'\n",
      "[2025-12-08 21:00:00,022] INFO: Saving current progress to github_graph.gexf\n",
      "[2025-12-08 21:00:00,268] INFO: Saved graph with 302 nodes and 11870 edges to github_graph.gexf\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Run the async function using asyncio\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     G, user_languages = \u001B[43masyncio\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrun_graph_building\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m     save_language_map(user_languages)\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\SG_rock_network\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001B[39m, in \u001B[36m_patch_asyncio.<locals>.run\u001B[39m\u001B[34m(main, debug)\u001B[39m\n\u001B[32m     28\u001B[39m task = asyncio.ensure_future(main)\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     32\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m task.done():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\SG_rock_network\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001B[39m, in \u001B[36m_patch_loop.<locals>.run_until_complete\u001B[39m\u001B[34m(self, future)\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f.done():\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m     97\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mEvent loop stopped before Future completed.\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m98\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:200\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    198\u001B[39m \u001B[38;5;28mself\u001B[39m.__log_traceback = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    199\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m200\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception.with_traceback(\u001B[38;5;28mself\u001B[39m._exception_tb)\n\u001B[32m    201\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001B[39m, in \u001B[36mTask.__step_run_and_handle_result\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m    300\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    301\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    302\u001B[39m         \u001B[38;5;66;03m# We use the `send` method directly, because coroutines\u001B[39;00m\n\u001B[32m    303\u001B[39m         \u001B[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m304\u001B[39m         result = \u001B[43mcoro\u001B[49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    305\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    306\u001B[39m         result = coro.throw(exc)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36mrun_graph_building\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# Start fresh\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     10\u001B[39m     logger.info(\u001B[33m\"\u001B[39m\u001B[33mBuilding new graph...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m     G, user_languages = \u001B[38;5;28;01mawait\u001B[39;00m build_graph(SEED_USER, depth=DEPTH, contrib_limit=CONTRIBUTOR_LIMIT,\n\u001B[32m     12\u001B[39m                          repo_limit=REPO_LIMIT, graph_file=GRAPH_FILE)\n\u001B[32m     14\u001B[39m     \u001B[38;5;66;03m# If the graph was saved during building due to rate limit, it's already in the right format\u001B[39;00m\n\u001B[32m     15\u001B[39m     \u001B[38;5;66;03m# Otherwise, save it now\u001B[39;00m\n\u001B[32m     16\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.exists(GRAPH_FILE):\n",
      "\u001B[31mValueError\u001B[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "async def get_repo_languages(session: aiohttp.ClientSession, owner: str, repo: str) -> Dict[str, int]:\n",
    "    url = f\"{BASE}/repos/{owner}/{repo}/languages\"\n",
    "    \n",
    "    try:\n",
    "        async with session.get(url, headers=HEADERS) as response:\n",
    "            if response.status == 200:\n",
    "                data = await response.json()\n",
    "                logger.debug(f\"Fetched languages for {owner}/{repo}: {list(data.keys())}\")\n",
    "                return data\n",
    "            elif response.status == 404:\n",
    "                logger.warning(f\"Repository {owner}/{repo} not found\")\n",
    "                return {}\n",
    "            elif response.status == 403:\n",
    "                remaining = response.headers.get('X-RateLimit-Remaining', '?')\n",
    "                if remaining == '0':\n",
    "                    reset_time = response.headers.get('X-RateLimit-Reset', '?')\n",
    "                    raise RateLimitExceeded(f\"Rate limit exceeded. Reset at {reset_time}\")\n",
    "                logger.warning(f\"Access forbidden for {owner}/{repo}\")\n",
    "                return {}\n",
    "            else:\n",
    "                logger.warning(f\"Failed to fetch languages for {owner}/{repo}: HTTP {response.status}\")\n",
    "                return {}\n",
    "    except aiohttp.ClientError as e:\n",
    "        logger.error(f\"Network error fetching languages for {owner}/{repo}: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "async def get_repo_readme(session: aiohttp.ClientSession, owner: str, repo: str, max_size: int = 50000) -> Optional[str]:\n",
    "\n",
    "    url = f\"{BASE}/repos/{owner}/{repo}/readme\"\n",
    "    \n",
    "    try:\n",
    "        async with session.get(url, headers=HEADERS) as response:\n",
    "            if response.status == 200:\n",
    "                data = await response.json()\n",
    "                # GitHub returns base64 encoded content\n",
    "                content_b64 = data.get(\"content\", \"\")\n",
    "                if content_b64:\n",
    "                    try:\n",
    "                        # Decode base64\n",
    "                        content = base64.b64decode(content_b64).decode('utf-8')\n",
    "                        # Truncate if too large\n",
    "                        if len(content) > max_size:\n",
    "                            logger.debug(f\"Truncating README for {owner}/{repo} from {len(content)} to {max_size} chars\")\n",
    "                            content = content[:max_size] + \"\\n...[truncated]\"\n",
    "                        logger.debug(f\"Fetched README for {owner}/{repo} ({len(content)} chars)\")\n",
    "                        return content\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Failed to decode README for {owner}/{repo}: {str(e)}\")\n",
    "                        return None\n",
    "                return None\n",
    "            elif response.status == 404:\n",
    "                logger.debug(f\"No README found for {owner}/{repo}\")\n",
    "                return None\n",
    "            elif response.status == 403:\n",
    "                remaining = response.headers.get('X-RateLimit-Remaining', '?')\n",
    "                if remaining == '0':\n",
    "                    reset_time = response.headers.get('X-RateLimit-Reset', '?')\n",
    "                    raise RateLimitExceeded(f\"Rate limit exceeded. Reset at {reset_time}\")\n",
    "                logger.warning(f\"Access forbidden for {owner}/{repo}\")\n",
    "                return None\n",
    "            else:\n",
    "                logger.warning(f\"Failed to fetch README for {owner}/{repo}: HTTP {response.status}\")\n",
    "                return None\n",
    "    except aiohttp.ClientError as e:\n",
    "        logger.error(f\"Network error fetching README for {owner}/{repo}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "async def fetch_repo_data(session: aiohttp.ClientSession, repo: str, max_readme_size: int = 50000) -> tuple:\n",
    "\n",
    "    owner, repo_name = repo.split(\"/\", 1)\n",
    "    \n",
    "    # Fetch languages and README in parallel\n",
    "    lang_task = get_repo_languages(session, owner, repo_name)\n",
    "    readme_task = get_repo_readme(session, owner, repo_name, max_readme_size)\n",
    "    \n",
    "    lang_data, readme = await asyncio.gather(lang_task, readme_task)\n",
    "    \n",
    "    # Determine primary language\n",
    "    primary_lang = None\n",
    "    if lang_data:\n",
    "        primary_lang = max(lang_data, key=lang_data.get)\n",
    "    \n",
    "    data = {\n",
    "        \"languages\": lang_data,\n",
    "        \"primary_language\": primary_lang,\n",
    "        \"readme\": readme,\n",
    "        \"total_bytes\": sum(lang_data.values()) if lang_data else 0\n",
    "    }\n",
    "    \n",
    "    return repo, data\n",
    "\n",
    "def save_json(data: dict, filename: str):\n",
    "    \"\"\"Save data to JSON file.\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    logger.info(f\"Saved data to {filename}\")\n",
    "\n",
    "def load_json(filename: str) -> dict:\n",
    "    \"\"\"Load data from JSON file.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        return {}\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_repos_from_graph(G) -> Set[str]:\n",
    "    all_repos = set()\n",
    "    \n",
    "    for node, data in G.nodes(data=True):\n",
    "        repos_str = data.get(\"repos\", \"\")\n",
    "        if repos_str:\n",
    "            # Handle both comma-separated strings and single repos\n",
    "            if \",\" in repos_str:\n",
    "                repos = [r.strip() for r in repos_str.split(\",\")]\n",
    "            else:\n",
    "                repos = [repos_str.strip()]\n",
    "            \n",
    "            # Only add valid \"owner/repo\" format\n",
    "            for repo in repos:\n",
    "                if repo and \"/\" in repo:\n",
    "                    all_repos.add(repo)\n",
    "    \n",
    "    logger.info(f\"Extracted {len(all_repos)} unique repositories from graph\")\n",
    "    return all_repos\n",
    "\n",
    "async def collect_repo_data(G, output_file: str = \"repo_data.json\", \n",
    "                           progress_file: str = \"repo_data_progress.json\",\n",
    "                           max_readme_size: int = 50000,\n",
    "                           batch_size: int = 50,\n",
    "                           max_concurrent: int = 10):\n",
    "    # Extract unique repos from graph\n",
    "    all_repos = extract_repos_from_graph(G)\n",
    "    all_repos = sorted(all_repos)  # Sort for consistent ordering\n",
    "    \n",
    "    # Load any existing progress\n",
    "    repo_data = load_json(progress_file)\n",
    "    already_processed = set(repo_data.keys())\n",
    "    remaining_repos = [r for r in all_repos if r not in already_processed]\n",
    "    \n",
    "    if already_processed:\n",
    "        logger.info(f\"Resuming from progress file: {len(already_processed)} repos already processed\")\n",
    "    \n",
    "    logger.info(f\"Need to process {len(remaining_repos)} repositories\")\n",
    "    logger.info(f\"Using batch_size={batch_size}, max_concurrent={max_concurrent}\")\n",
    "    \n",
    "    if not remaining_repos:\n",
    "        logger.info(\"All repos already processed!\")\n",
    "        save_json(repo_data, output_file)\n",
    "        return repo_data\n",
    "    \n",
    "    # Create semaphore to limit concurrent requests\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def fetch_with_semaphore(session, repo):\n",
    "        \"\"\"Wrapper to limit concurrent requests.\"\"\"\n",
    "        async with semaphore:\n",
    "            try:\n",
    "                return await fetch_repo_data(session, repo, max_readme_size)\n",
    "            except RateLimitExceeded:\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {repo}: {str(e)}\")\n",
    "                return repo, None\n",
    "    \n",
    "    # Process repos in batches\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for batch_start in range(0, len(remaining_repos), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(remaining_repos))\n",
    "            batch = remaining_repos[batch_start:batch_end]\n",
    "            \n",
    "            logger.info(f\"Processing batch {batch_start//batch_size + 1}/{(len(remaining_repos)-1)//batch_size + 1} ({len(batch)} repos)\")\n",
    "            \n",
    "            try:\n",
    "                # Fetch all repos in batch concurrently\n",
    "                tasks = [fetch_with_semaphore(session, repo) for repo in batch]\n",
    "                results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "                \n",
    "                # Process results\n",
    "                for result in results:\n",
    "                    if isinstance(result, Exception):\n",
    "                        if isinstance(result, RateLimitExceeded):\n",
    "                            logger.error(f\"Rate limit exceeded: {str(result)}\")\n",
    "                            logger.info(\"Saving progress before stopping...\")\n",
    "                            save_json(repo_data, progress_file)\n",
    "                            logger.info(f\"Progress saved to {progress_file}. Run again to resume.\")\n",
    "                            return repo_data\n",
    "                        else:\n",
    "                            logger.error(f\"Unexpected exception in batch: {str(result)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    repo_name, data = result\n",
    "                    if data is not None:\n",
    "                        repo_data[repo_name] = data\n",
    "                \n",
    "                # Save progress after each batch\n",
    "                save_json(repo_data, progress_file)\n",
    "                processed_count = len(repo_data) - len(already_processed)\n",
    "                logger.info(f\"Progress: {processed_count}/{len(remaining_repos)} repos processed ({100*processed_count/len(remaining_repos):.1f}%)\")\n",
    "                \n",
    "                # Small delay between batches to be nice to the API\n",
    "                await asyncio.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing batch: {str(e)}\")\n",
    "                save_json(repo_data, progress_file)\n",
    "                continue\n",
    "    \n",
    "    # Save final results\n",
    "    save_json(repo_data, output_file)\n",
    "    logger.info(f\"Complete! Processed {len(repo_data)} repositories\")\n",
    "    logger.info(f\"Final data saved to {output_file}\")\n",
    "    \n",
    "    # Clean up progress file\n",
    "    if os.path.exists(progress_file):\n",
    "        os.remove(progress_file)\n",
    "        logger.info(f\"Removed progress file {progress_file}\")\n",
    "    \n",
    "    return repo_data\n",
    "\n",
    "def generate_user_languages(G, repo_data: dict, output_file: str = \"user_languages.json\"):\n",
    "    user_languages = {}\n",
    "    \n",
    "    for node, data in G.nodes(data=True):\n",
    "        repos_str = data.get(\"repos\", \"\")\n",
    "        if not repos_str:\n",
    "            continue\n",
    "        \n",
    "        # Parse repos for this user\n",
    "        if \",\" in repos_str:\n",
    "            repos = [r.strip() for r in repos_str.split(\",\")]\n",
    "        else:\n",
    "            repos = [repos_str.strip()]\n",
    "        \n",
    "        # Collect languages from their repos\n",
    "        languages = set()\n",
    "        for repo in repos:\n",
    "            if repo in repo_data:\n",
    "                repo_langs = repo_data[repo].get(\"languages\", {})\n",
    "                languages.update(repo_langs.keys())\n",
    "        \n",
    "        if languages:\n",
    "            user_languages[node] = sorted(languages)\n",
    "    \n",
    "    save_json(user_languages, output_file)\n",
    "    logger.info(f\"Generated language mapping for {len(user_languages)} users\")\n",
    "    \n",
    "    return user_languages"
   ],
   "id": "6cad330b34978896"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "G = nx.read_gexf('github_graph.gexf')\n",
    "# Collect repo data (with parallel processing)\n",
    "repo_data = await collect_repo_data(\n",
    "    G,\n",
    "    output_file=\"repo_data.json\",\n",
    "    progress_file=\"repo_data_progress.json\",\n",
    "    batch_size=100,          # Process 100 repos per batch\n",
    "    max_concurrent=20        # Max 20 concurrent API requests\n",
    ")"
   ],
   "id": "2fa92dd275797f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Github Connectivity project",
   "id": "c148035b0d4371ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read the graph\n",
    "G = nx.read_gexf('github_graph.gexf')\n",
    "print(\"Original network:\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n",
    "print(f\"  Connected: {nx.is_connected(G)}\")\n",
    "\n",
    "# If not connected, keep only largest connected component\n",
    "if not nx.is_connected(G):\n",
    "    print(\"\\nNetwork is not connected. Extracting largest component...\")\n",
    "    components = list(nx.connected_components(G))\n",
    "    print(f\"  Number of components: {len(components)}\")\n",
    "\n",
    "    # Get sizes of all components\n",
    "    component_sizes = sorted([len(c) for c in components], reverse=True)\n",
    "    print(f\"  Largest component: {component_sizes[0]} nodes\")\n",
    "    print(f\"  Second largest: {component_sizes[1] if len(component_sizes) > 1 else 0} nodes\")\n",
    "\n",
    "    # Keep largest component\n",
    "    largest_cc = max(components, key=len)\n",
    "    G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "    print(f\"\\nFiltered network (largest component only):\")\n",
    "    print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"  Edges: {G.number_of_edges()}\")\n",
    "    print(f\"  Connected: {nx.is_connected(G)}\")\n",
    "else:\n",
    "    print(\"  Graph is already connected!\")"
   ],
   "id": "f6d61e4059ae8473"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Network stats",
   "id": "bb7e41c65ca399d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Network density",
   "id": "f766a6df1f221d88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "density = nx.density(G)\n",
    "print(f\"Network density: {density:.4f}\")\n",
    "\n",
    "avg_clustering = nx.average_clustering(G.to_undirected())\n",
    "print(f\"Average clustering coefficient: {avg_clustering:.4f}\")"
   ],
   "id": "701a1fad7283b911"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Average degree",
   "id": "6ed79dcd1329f63d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "degrees = [G.degree(node) for node in G.nodes()]\n",
    "mean_deg = np.mean(degrees)\n",
    "median_deg = np.median(degrees)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(degrees, bins=50, alpha=0.7, edgecolor='black', linewidth=0.5, color='steelblue')\n",
    "plt.axvline(mean_deg, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_deg:.1f}')\n",
    "plt.axvline(median_deg, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_deg:.1f}')\n",
    "plt.xlabel('Degree (number of connections)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Degree Distribution in GitHub Developer Network', fontsize=13)\n",
    "plt.legend(fontsize=10, loc='upper right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.xlim(0, 500)  # Zoom to where most of the data is\n",
    "\n",
    "# Add note about long tail\n",
    "plt.text(0.98, 0.8, f'Long tail extends to {max(degrees)}',\n",
    "         transform=plt.gca().transAxes,\n",
    "         fontsize=9, verticalalignment='top', horizontalalignment='right',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('degree_distribution.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "f4afe4ef670c83dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Degree assortativity",
   "id": "13f3470ac00d9378"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "G_undirected = G\n",
    "assortativity = nx.degree_assortativity_coefficient(G_undirected)\n",
    "print(f\"Degree Assortativity Coefficient: {assortativity:.4f}\")"
   ],
   "id": "5d07472bd2bd8b26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Shortest path\n",
    "\n",
    "It is possible to find the shortest path between two users in the network. This is done by using the shortest_path function from NetworkX. In theory every user is connected to every other user. "
   ],
   "id": "791055aea013ef27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def shortest_path_between_users(G, source_user, target_user):\n",
    "    if source_user not in G:\n",
    "        raise ValueError(f\"User '{source_user}' not found in graph\")\n",
    "\n",
    "    if target_user not in G:\n",
    "        raise ValueError(f\"User '{target_user}' not found in graph\")\n",
    "\n",
    "    try:\n",
    "        path = nx.shortest_path(G, source=source_user, target=target_user)\n",
    "        return path\n",
    "    except nx.NetworkXNoPath:\n",
    "        raise nx.NetworkXNoPath(\n",
    "            f\"No path found from '{source_user}' to '{target_user}'\"\n",
    "        )\n",
    "\n",
    "\n",
    "shortest_path_between_users(G, 'Pakkutaq', 'torvalds')"
   ],
   "id": "99e885bd885578f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Communities",
   "id": "a320f75b2f16a50c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def detect_structural_communities(G):\n",
    "    communities = nx.community.louvain_communities(G, seed=123)\n",
    "    partition = {}\n",
    "    for comm_id, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            partition[node] = comm_id\n",
    "    modularity = nx.community.modularity(G, communities)\n",
    "    return communities, partition, modularity\n",
    "\n",
    "communities, partition, modularity = detect_structural_communities(G)\n",
    "\n",
    "print(f\"Found {len(communities)} communities\")\n",
    "print(f\"Modularity: {modularity:.4f}\")\n",
    "\n",
    "# Get community sizes\n",
    "sizes = [(i, len(comm)) for i, comm in enumerate(communities)]\n",
    "sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop 10 communities:\")\n",
    "for rank, (comm_id, size) in enumerate(sizes[:10], 1):\n",
    "    pct = (size / len(partition)) * 100\n",
    "    print(f\"  {rank}. Community {comm_id}: {size} nodes ({pct:.1f}%)\")\n"
   ],
   "id": "37226ada2365d0ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Languages per community",
   "id": "5fdf3ae65ad3fc30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_community_languages(communities, partition, repo_data):\n",
    "    \"\"\"\n",
    "    Find dominant languages for each community.\n",
    "    \"\"\"\n",
    "    community_languages = {}\n",
    "\n",
    "    for comm_id in range(len(communities)):\n",
    "        # Get all users in this community\n",
    "        users = [node for node, c in partition.items() if c == comm_id]\n",
    "\n",
    "        # Count language bytes for this community\n",
    "        lang_bytes = {}\n",
    "        for user in users:\n",
    "            # Get repos for this user from G graph\n",
    "            repos_str = G.nodes[user].get(\"repos\", \"\")\n",
    "            if repos_str:\n",
    "                repos = [r.strip() for r in repos_str.split(\",\")]\n",
    "                for repo in repos:\n",
    "                    if repo in repo_data:\n",
    "                        languages = repo_data[repo].get(\"languages\", {})\n",
    "                        for lang, bytes_count in languages.items():\n",
    "                            if lang not in lang_bytes:\n",
    "                                lang_bytes[lang] = 0\n",
    "                            lang_bytes[lang] += bytes_count\n",
    "\n",
    "        community_languages[comm_id] = lang_bytes\n",
    "\n",
    "    return community_languages\n",
    "\n",
    "\n",
    "with open(\"repo_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    repo_data = json.load(f)\n",
    "# Analyze languages per community\n",
    "comm_langs = analyze_community_languages(communities, partition, repo_data)\n",
    "\n",
    "# Show top 3 languages for top 10 communities\n",
    "print(\"Top languages per community:\\n\")\n",
    "for rank, (comm_id, size) in enumerate(sizes[:10], 1):\n",
    "    langs = comm_langs[comm_id]\n",
    "    total = sum(langs.values())\n",
    "    top_langs = sorted(langs.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    print(f\"{rank}. Community {comm_id} ({size} nodes):\")\n",
    "    for lang, bytes_count in top_langs:\n",
    "        pct = (bytes_count / total * 100) if total > 0 else 0\n",
    "        print(f\"   - {lang}: {pct:.1f}%\")\n",
    "    print()"
   ],
   "id": "3bf2b2fe907025a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def normalize_languages(comm_langs):\n",
    "    \"\"\"\n",
    "    Group related languages together.\n",
    "    - Java + Scala + Kotlin -> Java (JVM languages)\n",
    "    - C + C++ -> C/C++\n",
    "    \"\"\"\n",
    "    language_groups = {\n",
    "        # JVM languages\n",
    "        'Java': 'Java',\n",
    "        'Scala': 'Java',\n",
    "        'Kotlin': 'Java',\n",
    "\n",
    "        # C family\n",
    "        'C': 'C/C++',\n",
    "        'C++': 'C/C++',\n",
    "    }\n",
    "\n",
    "    normalized_comm_langs = {}\n",
    "\n",
    "    for comm_id, langs in comm_langs.items():\n",
    "        normalized_langs = {}\n",
    "\n",
    "        for lang, bytes_count in langs.items():\n",
    "            # Map to group or keep original\n",
    "            normalized_lang = language_groups.get(lang, lang)\n",
    "\n",
    "            if normalized_lang not in normalized_langs:\n",
    "                normalized_langs[normalized_lang] = 0\n",
    "            normalized_langs[normalized_lang] += bytes_count\n",
    "\n",
    "        normalized_comm_langs[comm_id] = normalized_langs\n",
    "\n",
    "    return normalized_comm_langs\n",
    "\n",
    "# Normalize languages before calculating purity\n",
    "comm_langs_normalized = normalize_languages(comm_langs)\n",
    "\n",
    "def calculate_language_purity(comm_langs, top_k=10):\n",
    "    \"\"\"\n",
    "    Calculate purity for each community.\n",
    "    Purity = percentage of bytes in the dominant language.\n",
    "    \"\"\"\n",
    "    purity_scores = {}\n",
    "\n",
    "    for comm_id, langs in comm_langs.items():\n",
    "        if not langs:\n",
    "            continue\n",
    "\n",
    "        total = sum(langs.values())\n",
    "        max_bytes = max(langs.values())\n",
    "        purity = (max_bytes / total * 100) if total > 0 else 0\n",
    "        dominant_lang = max(langs, key=langs.get)\n",
    "\n",
    "        purity_scores[comm_id] = {\n",
    "            'purity': purity,\n",
    "            'dominant_language': dominant_lang,\n",
    "            'total_bytes': total\n",
    "        }\n",
    "\n",
    "    return purity_scores\n",
    "\n",
    "# Use normalized languages\n",
    "purity_scores = calculate_language_purity(comm_langs_normalized)\n",
    "\n",
    "# Show purity for top communities\n",
    "print(\"\\nLanguage Purity for Top  Communities (with language grouping):\")\n",
    "print(f\"{'Community':<15} {'Size':<10} {'Dominant Lang':<20} {'Purity'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for rank, (comm_id, size) in enumerate(sizes[:5], 1):\n",
    "    if comm_id in purity_scores:\n",
    "        info = purity_scores[comm_id]\n",
    "        print(f\"Community {comm_id:<5} {size:<10} {info['dominant_language']:<20} {info['purity']:.1f}%\")\n",
    "\n",
    "# Calculate average purity across top communities\n",
    "top_10_ids = [comm_id for comm_id, _ in sizes]\n",
    "avg_purity = np.mean([purity_scores[cid]['purity'] for cid in top_10_ids if cid in purity_scores])\n",
    "print(f\"\\nAverage purity across communities: {avg_purity:.1f}%\")"
   ],
   "id": "c9b9224d94a852b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_community_centrality(G, partition, top_communities, top_n=5):\n",
    "    \"\"\"\n",
    "    Calculate centrality within each community's subgraph.\n",
    "    Returns top N developers by degree and betweenness for each community.\n",
    "    \"\"\"\n",
    "    community_leaders = {}\n",
    "\n",
    "    for comm_id, size in top_communities:\n",
    "        print(f\"\\nAnalyzing Community {comm_id} ({size} nodes)...\")\n",
    "\n",
    "        # Get subgraph for this community\n",
    "        comm_nodes = [node for node, c in partition.items() if c == comm_id]\n",
    "        subgraph = G.subgraph(comm_nodes).copy()\n",
    "\n",
    "        # Calculate centralities (fast on subgraphs!)\n",
    "        degree_cent = nx.degree_centrality(subgraph)\n",
    "        betweenness_cent = nx.betweenness_centrality(subgraph)\n",
    "\n",
    "        # Get top N\n",
    "        top_degree = sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        top_between = sorted(betweenness_cent.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "        community_leaders[comm_id] = {\n",
    "            'size': size,\n",
    "            'top_degree': top_degree,\n",
    "            'top_betweenness': top_between\n",
    "        }\n",
    "\n",
    "        print(f\"  Top {top_n} by degree: {[user for user, _ in top_degree]}\")\n",
    "        print(f\"  Top {top_n} by betweenness: {[user for user, _ in top_between]}\")\n",
    "\n",
    "    return community_leaders\n",
    "\n",
    "# Analyze top 5 communities\n",
    "top_5_communities = sizes[:5]\n",
    "community_leaders = analyze_community_centrality(G, partition, top_5_communities, top_n=5)\n"
   ],
   "id": "602f34b2eac9bb6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Get top 5 communities\n",
    "top_5 = sizes[:5]\n",
    "comm_ids = [cid for cid, _ in top_5]\n",
    "\n",
    "# Prepare data\n",
    "labels = [f\"Community {cid}\" for cid in comm_ids]\n",
    "all_languages = set()\n",
    "for cid in comm_ids:\n",
    "    all_languages.update(comm_langs_normalized[cid].keys())\n",
    "\n",
    "# Get top languages across all communities for coloring\n",
    "lang_totals = {}\n",
    "for cid in comm_ids:\n",
    "    for lang, bytes in comm_langs_normalized[cid].items():\n",
    "        lang_totals[lang] = lang_totals.get(lang, 0) + bytes\n",
    "\n",
    "top_langs = sorted(lang_totals.keys(), key=lambda x: lang_totals[x], reverse=True)[:8]\n",
    "other_langs = set(all_languages) - set(top_langs)\n",
    "\n",
    "# Build percentage data\n",
    "data = {lang: [] for lang in top_langs}\n",
    "data['Other'] = []\n",
    "\n",
    "for cid in comm_ids:\n",
    "    total = sum(comm_langs_normalized[cid].values())\n",
    "    for lang in top_langs:\n",
    "        pct = (comm_langs_normalized[cid].get(lang, 0) / total * 100) if total > 0 else 0\n",
    "        data[lang].append(pct)\n",
    "\n",
    "    # Other category\n",
    "    other_sum = sum(comm_langs_normalized[cid].get(lang, 0) for lang in other_langs)\n",
    "    other_pct = (other_sum / total * 100) if total > 0 else 0\n",
    "    data['Other'].append(other_pct)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Colors for languages\n",
    "colors = {\n",
    "    'Python': '#3776ab',\n",
    "    'Java': '#b07219',\n",
    "    'C/C++': '#555555',\n",
    "    'TypeScript': '#2b7489',\n",
    "    'JavaScript': '#f1e05a',\n",
    "    'PHP': '#4F5D95',\n",
    "    'Go': '#00ADD8',\n",
    "    'Ruby': '#701516',\n",
    "    'Other': '#cccccc'\n",
    "}\n",
    "\n",
    "# Create stacked bars\n",
    "bottom = np.zeros(len(comm_ids))\n",
    "for lang in top_langs + ['Other']:\n",
    "    values = data[lang]\n",
    "    ax.barh(labels, values, left=bottom, label=lang,\n",
    "            color=colors.get(lang, '#999999'), edgecolor='white', linewidth=0.5)\n",
    "    bottom += values\n",
    "\n",
    "ax.set_xlabel('Percentage of Language Bytes (%)', fontsize=12)\n",
    "ax.set_title('Language Distribution in Top 5 Communities', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.1, 0.5), fontsize=10)\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "# Add purity percentages as text\n",
    "for i, cid in enumerate(comm_ids):\n",
    "    purity = purity_scores[cid]['purity']\n",
    "    ax.text(102, i, f\"{purity:.1f}%\", va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('community_language_alignment.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "1265dac5208ffccc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Wordclouds",
   "id": "2c0a36f66bc3cc15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_readme_texts(repo_data):\n",
    "    readme_texts = []\n",
    "\n",
    "    for repo, data in repo_data.items():\n",
    "        readme = data.get(\"readme\")\n",
    "        if readme:\n",
    "            readme_texts.append(readme)\n",
    "\n",
    "    print(f\"Found {len(readme_texts)} repos with READMEs\")\n",
    "    return readme_texts\n",
    "\n",
    "readmes = extract_readme_texts(repo_data)"
   ],
   "id": "a1fb51f8f8e97ac0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_readme_text(text):\n",
    "    \"\"\"\n",
    "    Clean a single README text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove markdown links [text](url)\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)\n",
    "\n",
    "    # Remove code blocks (```)\n",
    "    text = re.sub(r'```[\\s\\S]*?```', '', text)\n",
    "\n",
    "    # Remove inline code (`)\n",
    "    text = re.sub(r'`[^`]*`', '', text)\n",
    "\n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_readmes(readme_texts):\n",
    "    \"\"\"\n",
    "    Clean all README texts.\n",
    "    \"\"\"\n",
    "    cleaned = [clean_readme_text(text) for text in readme_texts]\n",
    "    print(f\"Cleaned {len(cleaned)} READMEs\")\n",
    "    return cleaned\n",
    "\n",
    "# Clean the readmes\n",
    "cleaned_readmes = clean_readmes(readmes)"
   ],
   "id": "ffbf857a3642723c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_wordcloud(texts, title=\"Word Cloud\"):\n",
    "    \"\"\"\n",
    "    Create a word cloud from a list of texts.\n",
    "\n",
    "    Parameters:\n",
    "        texts: list of strings\n",
    "        title: title for the plot\n",
    "    \"\"\"\n",
    "    # Combine all texts\n",
    "    combined_text = \" \".join(texts)\n",
    "\n",
    "    # Create word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        max_words=100,\n",
    "        random_state=42\n",
    "    ).generate(combined_text)\n",
    "\n",
    "    # Display\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create overall word cloud\n",
    "create_wordcloud(cleaned_readmes, \"All READMEs Word Cloud\")"
   ],
   "id": "c8c130ade5a52d83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TF-IDF",
   "id": "591773467d5732d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_community_readmes(community_id, partition, repo_data):\n",
    "    # Get users in this community\n",
    "    users = [node for node, comm in partition.items() if comm == community_id]\n",
    "\n",
    "    # Collect their READMEs\n",
    "    readmes = []\n",
    "    for user in users:\n",
    "        repos_str = G.nodes[user].get(\"repos\", \"\")\n",
    "        if repos_str:\n",
    "            repos = [r.strip() for r in repos_str.split(\",\")]\n",
    "            for repo in repos:\n",
    "                if repo in repo_data:\n",
    "                    readme = repo_data[repo].get(\"readme\")\n",
    "                    if readme:\n",
    "                        readmes.append(readme)\n",
    "\n",
    "    return readmes\n",
    "\n",
    "def analyze_community_tfidf(community_id, partition, repo_data, top_n=10):\n",
    "    \"\"\"\n",
    "    Find top TF-IDF terms for a specific community.\n",
    "    \"\"\"\n",
    "    # Get READMEs for this community\n",
    "    readmes = get_community_readmes(community_id, partition, repo_data)\n",
    "\n",
    "    if not readmes:\n",
    "        return []\n",
    "\n",
    "    # Clean the READMEs\n",
    "    cleaned = [clean_readme_text(text) for text in readmes]\n",
    "\n",
    "    # Combine into one document for this community\n",
    "    community_text = \" \".join(cleaned)\n",
    "\n",
    "    return community_text, len(readmes)\n",
    "\n",
    "# Collect texts for top 5 communities\n",
    "print(\"Collecting README texts for top communities...\\n\")\n",
    "\n",
    "community_texts = {}\n",
    "for rank, (comm_id, size) in enumerate(sizes[:6], 1):\n",
    "    text, readme_count = analyze_community_tfidf(comm_id, partition, repo_data)\n",
    "    community_texts[comm_id] = text\n",
    "    print(f\"Community {comm_id}: {readme_count} READMEs\")"
   ],
   "id": "2f50a953c53b39da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compare_communities_tfidf(community_texts, top_n=15):\n",
    "    # Prepare documents (one per community)\n",
    "    comm_ids = list(community_texts.keys())\n",
    "    documents = [community_texts[cid] for cid in comm_ids]\n",
    "\n",
    "    # Custom stopwords\n",
    "    custom_stopwords = list(STOPWORDS)\n",
    "    custom_stopwords.extend([\n",
    "        'use', 'used', 'using', 'run', 'file', 'will', 'one', 'two',\n",
    "        'project', 'new', 'set', 'may', 'also', 'see', 'get', 'make',\n",
    "        'example', 'install', 'start', 'first', 'need', 'add', 'name',\n",
    "        'following', 'via', 'note', 'number', 'type', 'value', 'result',\n",
    "        'allow', 'include', 'provide', 'contain', 'based', 'support'\n",
    "    ])\n",
    "\n",
    "    # Compute TF-IDF\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        stop_words=custom_stopwords,\n",
    "        min_df=1,\n",
    "        ngram_range=(1, 2)  # Include bigrams\n",
    "    )\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Get top terms for each community\n",
    "    print(f\"{'Community':<15} {'Top Distinctive Terms'}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for idx, comm_id in enumerate(comm_ids):\n",
    "        # Get TF-IDF scores for this community\n",
    "        scores = tfidf_matrix[idx].toarray()[0]\n",
    "        top_indices = scores.argsort()[-top_n:][::-1]\n",
    "        top_terms = [feature_names[i] for i in top_indices]\n",
    "\n",
    "        # Get dominant language for context\n",
    "        if comm_id in comm_langs_normalized:\n",
    "            langs = comm_langs_normalized[comm_id]\n",
    "            top_lang = max(langs, key=langs.get)\n",
    "        else:\n",
    "            top_lang = \"Unknown\"\n",
    "\n",
    "        print(f\"\\nCommunity {comm_id:<5} ({top_lang}):\")\n",
    "        print(f\"  {', '.join(top_terms)}\")\n",
    "\n",
    "# Run TF-IDF comparison\n",
    "compare_communities_tfidf(community_texts, top_n=15)"
   ],
   "id": "2d358bcff274c6db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_community_wordclouds(community_texts):\n",
    "    # Custom stopwords\n",
    "    custom_stopwords = STOPWORDS.copy()\n",
    "    custom_stopwords.update([\n",
    "        'use', 'used', 'using', 'run', 'file', 'will', 'one', 'two',\n",
    "        'project', 'new', 'set', 'may', 'also', 'see', 'get', 'make',\n",
    "        'example', 'install', 'start', 'first', 'need', 'add', 'name',\n",
    "        'following', 'via', 'note', 'number', 'type', 'value', 'result',\n",
    "        'allow', 'include', 'provide', 'contain', 'based', 'support'\n",
    "    ])\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, (comm_id, text) in enumerate(community_texts.items()):\n",
    "        # Get dominant language\n",
    "        if comm_id in comm_langs_normalized:\n",
    "            langs = comm_langs_normalized[comm_id]\n",
    "            top_lang = max(langs, key=langs.get)\n",
    "            purity = (langs[top_lang] / sum(langs.values()) * 100)\n",
    "        else:\n",
    "            top_lang = \"Unknown\"\n",
    "            purity = 0\n",
    "\n",
    "        # Create wordcloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=600,\n",
    "            height=400,\n",
    "            background_color='white',\n",
    "            max_words=50,\n",
    "            stopwords=custom_stopwords,\n",
    "            colormap='viridis'\n",
    "        ).generate(text)\n",
    "\n",
    "        # Plot\n",
    "        axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[idx].axis('off')\n",
    "        axes[idx].set_title(f'Community {comm_id}\\n({top_lang}, {purity:.1f}% purity)',\n",
    "                           fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Hide extra subplot if we have fewer than 6 communities\n",
    "    if len(community_texts) < 7:\n",
    "        for idx in range(len(community_texts), 6):\n",
    "            axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create wordclouds\n",
    "create_community_wordclouds(community_texts)"
   ],
   "id": "30b73dde1be85bff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6 degrees of separation experiments",
   "id": "3aee55ca8bf41229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def test_small_world(G, num_samples=10000):\n",
    "    nodes = list(G.nodes())\n",
    "    path_lengths = []\n",
    "    no_path_count = 0\n",
    "\n",
    "    print(f\"Testing {num_samples} random node pairs...\\n\")\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Pick two random nodes\n",
    "        node1, node2 = random.sample(nodes, 2)\n",
    "\n",
    "        try:\n",
    "            # Use your existing function\n",
    "            path = shortest_path_between_users(G, node1, node2)\n",
    "            path_length = len(path) - 1  # Number of edges\n",
    "            path_lengths.append(path_length)\n",
    "\n",
    "            if i < 10:  # Show first 10 examples\n",
    "                print(f\"{i+1}. {node1}  {node2}: {path_length} steps\")\n",
    "\n",
    "        except (nx.NetworkXNoPath, ValueError):\n",
    "            no_path_count += 1\n",
    "            if i < 10:\n",
    "                print(f\"{i+1}. {node1}  {node2}: No path (disconnected)\")\n",
    "\n",
    "    # Statistics\n",
    "    if path_lengths:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Results from {len(path_lengths)} connected pairs:\")\n",
    "        print(f\"  Average path length: {np.mean(path_lengths):.2f}\")\n",
    "        print(f\"  Median path length: {np.median(path_lengths):.1f}\")\n",
    "        print(f\"  Min path length: {min(path_lengths)}\")\n",
    "        print(f\"  Max path length: {max(path_lengths)}\")\n",
    "\n",
    "    return path_lengths\n",
    "\n",
    "# Run experiment\n",
    "path_lengths = test_small_world(G, num_samples=len(G.nodes()))"
   ],
   "id": "6cec370edc7dceda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Semantics analysis",
   "id": "689d3961ef8baf6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sentiment analysis setup (VADER)\n",
    "\n",
    "\n",
    "# Ensure the VADER lexicon is available\n",
    "try:\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "except LookupError:\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def compute_sentiment_scores(texts):\n",
    "    \"\"\"\n",
    "    Compute VADER compound sentiment scores for a list of texts.\n",
    "    Returns a NumPy array of scores in [-1, 1].\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for txt in texts:\n",
    "        if not txt:\n",
    "            continue\n",
    "        txt = txt.strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "        score = sia.polarity_scores(txt)[\"compound\"]\n",
    "        scores.append(score)\n",
    "    return np.array(scores, dtype=float)"
   ],
   "id": "f821d2dc1f37091b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Overall sentiment for all cleaned READMEscleaned_readmes = clean_readmes(readmes)\n",
    "overall_sentiment = compute_sentiment_scores(cleaned_readmes)\n",
    "\n",
    "n = len(overall_sentiment)\n",
    "print(f\"Overall README sentiment (n = {n})\")\n",
    "\n",
    "if n > 0:\n",
    "    mean_val = overall_sentiment.mean()\n",
    "    median_val = np.median(overall_sentiment)\n",
    "    min_val = overall_sentiment.min()\n",
    "    max_val = overall_sentiment.max()\n",
    "\n",
    "    # Simple polarity buckets\n",
    "    pos = (overall_sentiment > 0.05).sum()\n",
    "    neg = (overall_sentiment < -0.05).sum()\n",
    "    neu = n - pos - neg\n",
    "\n",
    "    print(f\"  mean       : {mean_val:.3f}\")\n",
    "    print(f\"  median     : {median_val:.3f}\")\n",
    "    print(f\"  min / max  : {min_val:.3f} / {max_val:.3f}\")\n",
    "    print(f\"  positive   : {pos} ({pos / n * 100:.1f}%)\")\n",
    "    print(f\"  neutral    : {neu} ({neu / n * 100:.1f}%)\")\n",
    "    print(f\"  negative   : {neg} ({neg / n * 100:.1f}%)\")\n",
    "else:\n",
    "    print(\"  No non-empty README texts to analyse.\")\n",
    "\n",
    "\n",
    "# Sentiment per community (top K structural communities)\n",
    "\n",
    "def compute_community_sentiment(partition, repo_data, sizes, top_k=5):\n",
    "    \"\"\"\n",
    "    For the top_k structural communities (by size), compute sentiment statistics\n",
    "    over the READMEs belonging to users in each community.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for rank, (comm_id, size) in enumerate(sizes[:top_k], 1):\n",
    "        # Get raw READMEs for this community (from existing helper)\n",
    "        readmes = get_community_readmes(comm_id, partition, repo_data)\n",
    "\n",
    "        # Clean them with your existing cleaner, then compute sentiment\n",
    "        cleaned = [clean_readme_text(t) for t in readmes if t]\n",
    "        scores = compute_sentiment_scores(cleaned)\n",
    "\n",
    "        if len(scores) == 0:\n",
    "            print(f\"Community {comm_id}: no README texts\")\n",
    "            continue\n",
    "\n",
    "        stats = {\n",
    "            \"rank\": rank,\n",
    "            \"nodes\": size,\n",
    "            \"n_readmes\": len(scores),\n",
    "            \"mean\": float(scores.mean()),\n",
    "            \"median\": float(np.median(scores)),\n",
    "            \"min\": float(scores.min()),\n",
    "            \"max\": float(scores.max()),\n",
    "            \"pos_frac\": float((scores > 0.05).sum() / len(scores)),\n",
    "            \"neg_frac\": float((scores < -0.05).sum() / len(scores)),\n",
    "        }\n",
    "        results[comm_id] = stats\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "community_sentiment = compute_community_sentiment(\n",
    "    partition,\n",
    "    repo_data,\n",
    "    sizes,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(\"\\nCommunity sentiment summary (top structural communities):\")\n",
    "for comm_id, stats in community_sentiment.items():\n",
    "    print(\n",
    "        f\"Community {comm_id} \"\n",
    "        f\"(rank {stats['rank']}, {stats['nodes']} nodes, {stats['n_readmes']} READMEs)\\n\"\n",
    "        f\"  mean={stats['mean']:.3f}, median={stats['median']:.3f}, \"\n",
    "        f\"min={stats['min']:.3f}, max={stats['max']:.3f}\\n\"\n",
    "        f\"  positive fraction={stats['pos_frac']:.2f}, \"\n",
    "        f\"negative fraction={stats['neg_frac']:.2f}\\n\"\n",
    "    )\n",
    "\n",
    "\n"
   ],
   "id": "a6a06021bd4eceb6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
